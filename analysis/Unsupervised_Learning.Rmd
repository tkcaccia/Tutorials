---
title: "Module 5: Unsupervised Learning"
author: "Stefano Cacciatore"
output: 
  html_document:
  toc: true
  toc_depth: 2
  theme: united
date: "`r format(Sys.Date(), '%B %d, %Y')`"
  
---

## Unsupervised Learning (UL)

Unsupervised Learning (UL) is a technique that uncovers patterns in data without predefined labels or extensive human input. Unlike supervised learning, which relies on data with known outcomes, UL focuses on exploring relationships within the data itself.

A key approach in UL is clustering, when data points are grouped based on their similarities. Common methods include **K-means clustering**, **Hierarchical clustering**, **Probabilistic clustering**. 

**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) is another powerful clustering method that identifies clusters based on data density and can handle noise effectively. 

For instance, in mRNA expression analysis, `clustering` can group genes with similar expression profiles, while `DBSCAN` might reveal clusters of genes with high expression density and identify outliers.

Additionally, dimensionality reduction techniques like **Principal Component Analysis (PCA)** and **t-Distributed Stochastic Neighbor Embedding (t-SNE)** are used to simplify and visualize complex data. These methods help reveal hidden structures and insights, such as the genetic basis of various conditions.

```{r,message=FALSE, warning=FALSE}
# Load necessary libraries
library(dplyr)
library(cluster)     # For clustering algorithms
library(factoextra)  # For cluster visualization
library(ggplot2)     # For data visualization
library(Rtsne)       # For t-SNE visualization
library(stats)       # For K-means
library(tidyr)       # For handling missing data
library(dendextend) # For Hierarchical Clustering
library(ggfortify)
library(dbscan)
library(mclust) # For probabilistic clustering
library(caret)  # For scaling
```

### 1. K-MEANS CLUSTERING

#### Step 1: Prepare Data

```{r}
data <- iris

# Remove "Species" column as this is not needed for unsupervised learning.
clean_data <- data[,c(1:4)]

# Step 1: Remove rows with missing values
clean_data <- na.omit(clean_data)

# Step 2: Normalization/Scaling
clean_data <- scale(clean_data)
```

#### Step 2: Data Filtering and Reduction

- This step involves filtering out variables with ***low variance*** and any ***missing values*** to ensure our data is clean and robust for analysis.

- The aim is to filter out variables that are ***unlikely to be informative*** for ***distinguishing between different samples***.

```{r}
# Step 1: Calculate the variance for each gene (row)
variances <- apply(clean_data, 1, var)

# Step 2: Set a threshold for filtering low variance genes
threshold <- quantile(variances, 0.25) # Lower 25% variance genes

# Step 3: Retain only the genes with variance above the threshold
filtered_data <- clean_data[variances > threshold, ]

# Step 4: Remove duplicate rows
filtered_data <- unique(filtered_data)
```

#### Step 3: Clustering Prep & Criterion Evaluation

- Determine the number of clusters and evaluation criteria to ensure meaningful clusters.

- Use methods like the elbow method, silhouette score, or cross-validation to decide the optimal number of clusters.

```{r}

# 1. Elbow method
fviz_nbclust(filtered_data, kmeans, method = "wss") + 
  labs(subtitle = "Elbow Method for Optimal K")

# 2.  Silhouette method 
fviz_nbclust(filtered_data, kmeans, method = "silhouette") + 
  labs(subtitle = "Silhouette Method for Optimal K")

```

#### Step 4: K-means Clustering & Parameter Selection

Perform K-means clustering with k = 2

```{r}
set.seed(123)
k <- 2
```

```{r}
# Use Euclidean distance
kmeans_res_euclidean <- kmeans(filtered_data, centers = k, nstart = 25)

# For comparison, use Manhattan distance
kmeans_res_manhattan <- kmeans(dist(filtered_data, method = "manhattan"), centers = k, nstart = 25)

# Visualize the clusters

fviz_cluster(kmeans_res_euclidean, data = filtered_data) + 
  labs(title = "K-means Clustering with Euclidean Distance")

fviz_cluster(kmeans_res_manhattan, data = filtered_data) + 
  labs(title = "K-means Clustering with Manhattan Distance")

```

#### Step 5: Visualization with PCA

- Simplify and visualize the dataset using PCA to understand the structure and separation of clusters.

- Principal Component Analysis (PCA): Reduces dimensionality while preserving variance, helping to visualize data clusters.

```{r}
# PCA Visualization
pca_res <- prcomp(filtered_data, scale. = TRUE)
fviz_pca_ind(pca_res, 
             geom.ind = "point", 
             col.ind = as.factor(kmeans_res_euclidean$cluster)) +
  labs(title = "PCA Visualization")
```

#### Step 6: Visualization with t-SNE

- Use t-SNE to visualize high-dimensional data in a lower-dimensional space.

- Captures complex nonlinear relationships and helps visualize clusters in a 2D or 3D space.

```{r, message=FALSE, warning=FALSE}
# t-SNE Visualization

set.seed(123)
tsne_res <- Rtsne(as.matrix(filtered_data), dims = 2, perplexity = 30, verbose = TRUE)

```

```{r}
# Convert t-SNE results to a data frame for plotting
tsne_data <- as.data.frame(tsne_res$Y)
colnames(tsne_data) <- c("Dim1", "Dim2")

# Map the clusters from k-means result to the t-SNE data
tsne_data$Cluster <- as.factor(kmeans_res_euclidean$cluster)

# Plot t-SNE results
library(ggplot2)
ggplot(tsne_data, aes(x = Dim1, y = Dim2, color = Cluster)) + 
  geom_point() + 
  labs(title = "t-SNE Visualization", x = "Dimension 1", y = "Dimension 2")
```

#### T-SNE Parameter optimization:

One could adjust parameters such as `perplexity`, `exaggeration`, and `PCAComps` iteratively based on the visualization results. 

Evaluate if clusters are *too separated* or *not well defined*. Modify exaggeration_num and perplexity to refine cluster separation and representation.

Lets see what happens when we adjust some parameters?

```{r}
# Set seed for reproducibility
set.seed(1)

# Example data: replace this with your actual data
expression <- filtered_data
expression_standardized <- scale(expression)  # Standardize data

# Parameters
algorithm <- 'barnes_hut'  # Not directly set in R, handled internally
distance_metric <- 'euclidean'  # Rtsne does not support 'spearman' directly
exaggeration_num <- 4
PCAComps <- 0
pca_comps <- if (PCAComps == 0) NULL else PCAComps  # Set PCA dimension reduction
perp_num <- 30

# Optionally apply PCA if PCAComps > 0
if (!is.null(pca_comps)) {
  pca_res <- prcomp(expression_standardized, center = TRUE, scale. = TRUE)
  expression_pca <- pca_res$x[, 1:pca_comps]
} else {
  expression_pca <- expression_standardized
}

# Run t-SNE
tsne_res <- Rtsne(expression_pca, dims = 2, pca = PCAComps > 0, perplexity = perp_num, 
                   check_duplicates = FALSE, verbose = TRUE)

# Convert t-SNE results to a data frame for plotting
tsne_data <- as.data.frame(tsne_res$Y)
colnames(tsne_data) <- c("Dim1", "Dim2")

# Here we use the cluster labels from k-means:
set.seed(1)
cluster_labels <- kmeans(expression_standardized, centers = 3)$cluster
tsne_data$Cluster <- as.factor(cluster_labels)

# Plotting t-SNE results
ggplot(tsne_data, aes(x = Dim1, y = Dim2, color = Cluster)) + 
  geom_point() + 
  labs(title = "t-SNE Visualization", x = "Dimension 1", y = "Dimension 2") +
  theme_minimal()
```

<details>

<summary>*Assessing Clustering Performance?*</summary>

***Within-Cluster Sum of Squares (Inertia)***

- Inertia measures the compactness of clusters. 
- Lower values indicate better clustering.

```{r}
# Compute inertia for K-means clustering
inertia_euclidean <- kmeans_res_euclidean$tot.withinss
inertia_manhattan <- kmeans_res_manhattan$tot.withinss

# Print inertia values
cat("Inertia for Euclidean K-means:", inertia_euclidean, "\n")
cat("Inertia for Manhattan K-means:", inertia_manhattan, "\n")

```

We see that our Euclidean K-means model produced more compact clusters.

***Silhouette Analysis***

- Silhouette scores measure how similar each point is to its own cluster compared to other clusters. 

- Scores close to 1 indicate good clustering.

```{r}
library(cluster)

# Calculate silhouette scores for Euclidean K-means
silhouette_euclidean <- silhouette(kmeans_res_euclidean$cluster, dist(filtered_data))
plot(silhouette_euclidean, main = "Silhouette Plot for Euclidean K-means")

# Calculate silhouette scores for Manhattan K-means
silhouette_manhattan <- silhouette(kmeans_res_manhattan$cluster, dist(filtered_data, method = "manhattan"))
plot(silhouette_manhattan, main = "Silhouette Plot for Manhattan K-means")

```

***Davies-Bouldin Index***

- The DB-index is a validation technique that is a metric for evaluating clustering models.

- The metric measures the average similarity between each cluster and the cluster most similar to it.

- Similarity is assessed as the ratio of how spread out the points are within a cluster to the distance between cluster centroids. 

- Low Davies-Bouldin Index values = better clustering, with well-separated clusters and lower dispersion being more favorable.

```{r}
# install.packages("clusterSim")
library(clusterSim)

# Step 1:   Prepare distance matrix for Euclidean distance clustering
dist_matrix <- dist(filtered_data)
###   Prepare distance matrix for Manhattan distance clustering
dist_matrix_manhattan <- dist(filtered_data, method = "manhattan")

###   Calculate Davies-Bouldin Index for Euclidean distance clustering:
db_index_euclidean <- index.DB(filtered_data, kmeans_res_euclidean$cluster, d =
                                 dist_matrix, centrotypes = "centroids")

# Compute Davies-Bouldin Index for our Manhattan distance clustering:
db_index_manhattan <- index.DB(filtered_data, kmeans_res_manhattan$cluster, d = dist_matrix_manhattan, centrotypes = "medoids")

# Print Index for both distances:
print(paste("Davies-Bouldin Index (Euclidean):", db_index_euclidean$DB))
print(paste("Davies-Bouldin Index (Manhattan):", db_index_manhattan$DB))
```

***Cluster Validation Metrics***

Additional metrics can provide further validation of clustering results.

```{r}
library(NbClust)

# Use NbClust to determine the optimal number of clusters
nb <- NbClust(filtered_data, min.nc = 2, max.nc = 10, method = "kmeans")

```

### 2. Hierarchical Clustering:

```{r}
# Using our cleaned data: "filtered_data"

# Compute the distance matrix
dist_matrix <- dist(filtered_data, method = "euclidean")

# Perform hierarchical clustering
hc <- hclust(dist_matrix, method = "complete")

# Plot the dendrogram
plot(hc, main = "Dendrogram of Hierarchical Clustering", xlab = "Branches", ylab = "Euclidean Distance", labels = FALSE)
```

Visualising the output from our Hierarchical Clustering using PCA:

```{r}
# Convert PCA results to a data frame for ggplot2
pca_df <- as.data.frame(pca_res$x)

# Perform Hierarchical Clustering
dist_matrix <- dist(filtered_data, method = "euclidean")
hc <- hclust(dist_matrix, method = "complete")

# Cut dendrogram to form clusters
clusters <- cutree(hc, k = 3)  # Adjust k based on the desired number of clusters

# Add cluster assignments to the PCA data
pca_res$clusters <- as.factor(clusters)

# Visualize the clusters using PCA for dimensionality reduction
ggplot(pca_res, aes(x = PC1, y = PC2, color = clusters)) +
  geom_point(size = 3) +
  labs(title = "PCA of Hierarchical Clustering", x = "PC1", y = "PC2") +
  theme_minimal()
```

### 3. Probabilistic Clustering:

```{r}
# Data Prep:
data(iris)
data <- iris[, 1:4]  # Use only numerical features
# Normalize/Scale the data
scaled_data <- scale(data)

# Fit the Gaussian Mixture Model
model <- Mclust(data)

```

The output from the model indicates that the GMM identified 2 clusters with the same shape and variance. 

- Cluster 1: Contains 50 data points.
- Cluster 2: Contains 100 data points.

```{r}
# View model summary
summary(model)

```

```{r}
# Plot the clustering results
plot(model, what = "classification")

# Predict cluster memberships for new data
new_data <- data[1:5, ]
predictions <- predict(model, new_data)
print(predictions$classification)
```


### 4. Density-Based Clustering (DBSCAN):

DBSCAN identifies clusters based on the density and noise of data points. It is particularly useful for finding clusters of arbitrary shape and handling high-noise datasets.

```{r}
# Load necessary libraries
library(dbscan)
library(ggplot2)

# Data Prep:
data("iris")
iris <- as.matrix(iris[, 1:4])
# Remove rows with missing values
clean_data <- na.omit(iris)
# Normalize/Scale the data
scaled_data <- scale(clean_data)

```

*Determine suitable DBSCAN parameters:*

- `eps` = size (radius) of the epsilon neighborhood

- Where there is a sudden spike (increase) in the `kNN` distance = points to the right of this spike are most likely outliers. 

- Choose this kNN distance as the 'eps'

```{r}
# Visualize the k-NN distance plot for eps parameter
kNNdistplot(scaled_data, minPts = 5)  
```

Add a line where approximately the noise starts. We see at `~0.8` the noise begins:

```{r}
kNNdistplot(scaled_data, minPts = 5)  
abline(h = 0.8, col = "red", lty = 2)  
```

Now we can perform DBSCAN clustering:

```{r}
# Perform DBSCAN clustering
eps <- 0.8  # Set eps based on the k-NN distance plot
minPts <- 5  # minPts is set to the number of features + 1

dbscan_result <- dbscan(scaled_data, eps = eps, minPts = minPts)
dbscan_result

# Add cluster assignments to the original data
airquality_clustered <- cbind(clean_data, cluster = as.factor(dbscan_result$cluster))

# Visualize the clusters using PCA for dimensionality reduction
pca <- prcomp(scaled_data, scale. = TRUE)
pca_df <- as.data.frame(pca$x)
pca_df$cluster <- as.factor(dbscan_result$cluster)

ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(size = 3) +
  labs(title = "PCA of DBSCAN Clustering", x = "PC1", y = "PC2") +
  theme_minimal()
```

